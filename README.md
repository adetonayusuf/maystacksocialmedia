# maystacksocialmedia


# ğŸ“Š Creator Economy Analytics with Azure, Databricks & Power BI

### ğŸ” **Project Title:**  
**Leveraging Social Media Analytics for Strategic Competitor and Niche Analysis in the Creator Economy**

---

## ğŸ“ Project Overview

This project focuses on building a **scalable end-to-end data pipeline** for ingesting, processing, transforming, and analyzing **social media insights** related to creators and their content performance. It is tailored for use cases like audience segmentation, content effectiveness, and competitor intelligence in the **telecommunications & media space**.

---

## ğŸ§± Architecture Overview

### ğŸ”§ Technologies Used
- **Azure Data Factory (ADF)** â€“ Data ingestion (REST APIs â†’ Data Lake)
- **Azure Data Lake Storage (Gen2)** â€“ Bronze, Silver, Gold storage zones
- **Azure Databricks** â€“ Data transformation using PySpark
- **Azure PostgreSQL Database** â€“ Data serving layer
- **Power BI** â€“ Reporting and dashboarding
- **GitHub** â€“ CI/CD version control (planned)
- **Optional**: Azure Monitor, Great Expectations (for future extensions)

---

## âœ… Key Deliverables

| Layer   | Description                                                                 |
|---------|-----------------------------------------------------------------------------|
| **Bronze** | Raw ingestion of social media data via REST API into Azure Data Lake      |
| **Silver** | Cleaned, deduplicated, typecast data using Apache Spark                   |
| **Gold**   | Aggregated fact and dimension tables ready for analysis                   |
| **SQL DB** | PostgreSQL database hosted on Azure containing gold tables                |
| **Power BI** | 2-page executive dashboard with KPIs, charts, filters, and deep dives   |

---

## ğŸ”„ End-to-End Steps

### ğŸ“Œ Step 1: API Ingestion via ADF (Initially Attempted)
- Created 4 **Copy Data** activities (Demographics, Engagement, Content, Competitors)
- Targeted endpoint: `https://mainstack-media-api.amdari.io`
- Stored results in: `mainstack/bronze/[table_name]` (as `.parquet`)
- **Note**: Later replaced with Databricks job when the API returned 502 Bad Gateway

### ğŸ“Œ Step 2: Created Ingestion Job in Databricks (Workaround)
- Built a **custom ingestion job in Databricks** using Python `requests` and PySpark
- Extracted data from each endpoint and saved it to `/mnt/bronze/`
- Converted JSON responses to DataFrames
- Mounted blob storage with SAS authentication

### ğŸ“Œ Step 3: Silver Layer Transformation
- Cleaned and deduplicated data using PySpark
- Applied `cast`, `dropDuplicates`, `withColumnRenamed`
- Saved outputs to `/mnt/silver/[table]`

### ğŸ“Œ Step 4: Gold Modeling
- Created **star schema**: `fact_engagement`, `fact_content`, `dim_creators`, `dim_time`, `dim_platforms`
- Computed fields like `engagement_score`, `engagement_rate`, `monetization_category`
- Stored to: `/mnt/gold`

### ğŸ“Œ Step 5: Export to PostgreSQL
```python
df_fact_engagement.write.jdbc(
    url=jdbc_url,
    table="fact_engagement",
    mode="overwrite",
    properties=connection_properties
)
```

### ğŸ“Œ Step 6: Power BI Visualization
- Connected Power BI Desktop to Azure PostgreSQL
- Created 2-page executive dashboard:
  - **Page 1**: Overview KPIs, monetization %, engagement trend
  - **Page 2**: Performance bands, creator segments, content heatmap
- DAX measures: `Engagement Rate`, `Monetized Content %`, `Performance Band`

---

## ğŸ“Š Sample Visuals (Power BI)
<details>
<summary>Click to expand</summary>

- Total Reach, Likes, Revenue KPIs  
- Engagement Rate over Time  
- Creators by Subscription Tier  
- Performance Band Treemap  
- Heatmap: Content Type Ã— Platform

</details>

---

## ğŸ“¦ Folder Structure

```bash
MainstackAnalytics/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ SilverTransformations.ipynb
â”‚   â””â”€â”€ GoldModeling_Notebook.ipynb
â”‚
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ ADF_ingestion_pipeline.json (deprecated)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_exports/
â”‚       â””â”€â”€ dim_creators.csv
â”‚
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ dashboard.pbix
â”‚
â””â”€â”€ README.md
```

---

## ğŸ” Security Notes
- SAS tokens used for secure access to Data Lake
- PostgreSQL credentials managed via Databricks secret scopes
- Optional integration: Azure Key Vault for enterprise-grade security

---

## ğŸš€ Future Improvements
- Add **Delta Lake** format for ACID-compliant tables
- Integrate **Great Expectations** for column-level validation
- Use **GitHub Actions** for deploying notebooks and pipelines
- Enable real-time streaming using Event Hubs or Kafka

---

## ğŸ“¬ Contact

> **Yusuf Adetona**  
[LinkedIn](https://www.linkedin.com/in/yusuf-adetona-hnd-bsc-aat-aca-acca-diploma-in-ifrs/) â€¢ [Portfolio](https://www.datascienceportfol.io/adetonayusuf)  
ğŸ“§ yustone003@yahoo.com  
ğŸ“ +234-706-205-6766
